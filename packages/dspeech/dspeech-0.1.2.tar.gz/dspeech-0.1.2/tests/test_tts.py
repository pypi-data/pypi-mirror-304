from dspeech import TTS
import torchaudio

# Initialize the TTS model
tts_handler = TTS(
    device="cuda",  # Device to use (e.g., "cuda" for GPU)
    target_sample_rate=24000,  # Target sample rate for the output audio
)

# Basic voice cloning
result = tts_handler.clone(
    ref_audio=torchaudio.load("tests/a.wav"),  # Reference audio (can be a file path or (Tensor, int))
    ref_text="人相异化对",  # Reference text (can also use dspeech.STT module for automatic speech-to-text conversion)
    gen_text_batches=["你好我叫小明", "我是一个AI", "我会说中文"],  # List of texts to generate audio for
    speed=1,  # Speech speed
    channel=-1,  # Channel for reference audio (-1 to merge all channels)
    remove_silence=True,  # Remove silence from the reference audio
    wave_path="tests/tts_output.wav",  # Path to save the generated audio (None disables saving)
    spectrogram_path="tests/tts_output.png",  # Path to save the spectrogram (None disables saving)
    concat=True  # Concatenate generated texts into a single output file
)

# Load speaker information from a directory structure
# The directory should follow this structure:
# <path>/ 
# ├── speaker1/
# │   ├── happy.wav
# │   ├── happy.txt
# │   ├── neutral.wav
# │   ├── neutral.txt
# Each speaker directory contains audio files and corresponding text files.

speaker_info = tts_handler.get_speaker_info("tests/speaker")
print(speaker_info)

# Voice cloning with emotion
# In this case, gen_text_batches should include [[<spk>_<emo>]] to specify speaker and emotion,
# where <spk> is the speaker name and <emo> is the emotion.
# Each text batch should contain only one [[spk_emo]] tag to avoid errors.

result = tts_handler.clone_with_emo(
    gen_text_batches=[
        "[[zhaosheng_angry]]你怎么能这样对我说话！太过分了！",  # Text with speaker and emotion
        "[[zhaosheng_whisper]]小心点，别让别人听见了，这是一件秘密的事情。",
        "[[zhaosheng_sad]]我真的很难过，事情完全不在我的控制中。"
    ],
    speaker_info=speaker_info,
    speed=1,
    channel=-1,
    remove_silence=True,
    wave_path="tests/tts_output_emo.wav",  # Path to save the emotional audio output
    spectrogram_path="tests/tts_output_emo.png"  # Path to save the spectrogram
)

# Multi-speaker and multi-emotion dialogue generation
# This example assumes multiple speaker directories exist in 'tests/speaker', each containing multiple audios with different emotions.

# Loading speaker information for multiple speakers
speaker_info = tts_handler.get_speaker_info("tests/speaker")
print(speaker_info)

# Generate a dialogue with multiple speakers and emotions
result = tts_handler.clone_with_emo(
    gen_text_batches=[
        # Example conversation generated by GPT-4
        "[[xieyukai_angry]]你怎么能这样！你居然忘了今天是什么日子？太过分了！",
        "[[xuekaixiang_angry]]你别这样！我哪有忘，我只是…啊…忙了一点！",
        "[[duanyibo_normal]]呃，那个，我觉得你们俩还是冷静一点吧…今天不是才刚过中午吗？",
        
        "[[xieyukai_sad]]忙？你永远都在忙，从来不顾我的感受…我真的很失望。",
        "[[xuekaixiang_sad]]唉，我也很累啊，你以为我不想陪你吗？但工作就是这么多。",
        "[[duanyibo_whisper]]嗯…我是不是不该在这儿啊？",

        "[[xieyukai_proud]]好吧！那我告诉你，我今天自己也安排了别的事情，我可不需要你记住每个日子！",
        "[[xuekaixiang_surprised]]你…你安排了别的事情？你怎么不早说！",
        "[[duanyibo_surprised]]啊？所以你们两个是…各玩各的吗？",

        "[[xieyukai_whisper]]不想吵了…我只想安静一会儿。",
        "[[xuekaixiang_whisper]]我也是…唉，真不知道我们怎么变成这样的。",
        "[[duanyibo_delighted]]嘿嘿，看你们冷静下来了，那咱们一起喝杯咖啡怎么样？",

        "[[xieyukai_delighted]]嗯，好吧…那就一起去喝杯咖啡，今天还算没彻底毁掉。",
        "[[xuekaixiang_normal]]好吧，喝杯咖啡也行，反正我已经习惯了加班。",
        "[[duanyibo_angry]]等等！你们两个竟然就这么和好了？刚才吵得那么凶，我以为有戏看呢！",
    ],
    speaker_info=speaker_info,
    speed=1,
    channel=-1,
    remove_silence=True,
    wave_path="tests/tts_output_emo.wav",  # Path to save the output
    spectrogram_path="tests/tts_output_emo.png"  # Path to save the spectrogram
)
