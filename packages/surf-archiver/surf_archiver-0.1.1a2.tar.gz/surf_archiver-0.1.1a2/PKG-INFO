Metadata-Version: 2.1
Name: surf-archiver
Version: 0.1.1a2
Summary: 
Author: Isaac Williams
Author-email: isaac.andrew.williams@gmail.com
Requires-Python: >=3.9,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: aio-pika (>=9.4.1,<10.0.0)
Requires-Dist: pydantic (>=2.7.1,<3.0.0)
Requires-Dist: pydantic-settings[yaml] (>=2.2.1,<3.0.0)
Requires-Dist: s3fs (>=2024.3.1,<2025.0.0)
Requires-Dist: typer (>=0.12.3,<0.13.0)
Description-Content-Type: text/markdown

# Surf archiver: CLI tool 

Surf-Archiver copies daily data from S3, bundling it into a per experiment per day
tar archive. 

It consists of two components: 
1) a CLI tool
2) a remote client which can execute CLI installed on a remote machine

The CLI tool can be installed via [pipx](https://github.com/pypa/pipx).


# Development

The easiest way to develop the tool is via docker. This ensures that you can connect
to the dependent services (e.g. s3, rabbitmq). The `prestart` container ensures that 
some test data is populated on s3.

The `surf-archiver-cli` container allows for the invocation of the cli tool. For example:

```bash
docker compose run --rm surf-archiver-cli
surf-archiver --help # See the available commands

surf-archiver archive 2000-01-01  # archive a specific day
cat /data/app.log  # check the logs
ls /data/  # See what data has been archived

```

