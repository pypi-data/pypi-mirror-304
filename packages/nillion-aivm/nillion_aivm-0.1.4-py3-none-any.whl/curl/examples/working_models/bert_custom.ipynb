{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries and modules\n",
    "This code imports a variety of libraries, including PyTorch, NumPy, and Hugging Face's Transformers for model handling. It also contains commented-out code related to Curl and ONNX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import curl\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer,\n",
    "    pipeline,\n",
    "    AutoModel,\n",
    ")\n",
    "# curl.init()\n",
    "# from optimum.onnxruntime import ORTModelForSequenceClassification, ORTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Loading a Pretrained Model\n",
    "Here, we define `MODEL_NAME` with various BERT-based models for sequence classification. We load the model using Hugging Face's `AutoModelForSequenceClassification` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"prajjwal1/bert-tiny\"\n",
    "MODEL_NAME = \"gaunernst/bert-tiny-uncased\"\n",
    "#MODEL_NAME = \"optimum/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "MODEL_NAME = \"mrm8488/bert-tiny-finetuned-sms-spam-detection\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Input Text\n",
    "We prepare some input text and tokenize it using the model's tokenizer. The output tokens will be fed into the model for further processing. The `max_length` is set to the model's hidden size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT = \"Hello, my dog is cute\"\n",
    "hidden_size = model.config.hidden_size\n",
    "inputs = tokenizer(\n",
    "    INPUT_TEXT,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=hidden_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Tokenized Input\n",
    "Here, we look at the tokenized representation of our input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch to ONNX Conversion Function\n",
    "This function converts a PyTorch model to ONNX format. It takes the PyTorch model, a tokenizer, and an optional max length for tokenization as arguments. We use a dummy input to simulate real input data during conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, onnx\n",
    "\n",
    "def convert_pytorch_to_onnx_with_tokenizer(model, tokenizer, max_length=hidden_size):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch model to ONNX format, using tokenizer output as input.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dummy_input = \"This is a sample input text for ONNX conversion.\"\n",
    "    inputs = tokenizer(\n",
    "        dummy_input,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_names = [\"input_ids\", \"attention_mask\"]\n",
    "    onnx_file_path = tempfile.mktemp(suffix=\".onnx\")\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        tuple(inputs[k] for k in input_names),\n",
    "        onnx_file_path,\n",
    "        opset_version=20,\n",
    "        do_constant_folding=True,\n",
    "        input_names=input_names,\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={name: {0: \"batch_size\"} for name in input_names},\n",
    "    )\n",
    "    print(f\"Model exported to {onnx_file_path}\")\n",
    "    return onnx_file_path, input_names\n",
    "\n",
    "onnx_file_path, input_names = convert_pytorch_to_onnx_with_tokenizer(\n",
    "    model, tokenizer, max_length=hidden_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the ONNX Model\n",
    "Once we have converted the PyTorch model into ONNX format, we can run the ONNX model using the ONNX Runtime. This code prepares inputs for ONNX and asserts that the ONNX model produces outputs similar to the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_outputs = model(**inputs).logits\n",
    "expected_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Classifying Text Samples with ONNX\n",
    "This function takes a sentence, tokenizes it, runs it through the ONNX model, and compares the ONNX model's outputs with the PyTorch model's expected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "def classify_sample(sentence, model, onnx_file_path, input_names):\n",
    "    inputs = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=hidden_size,\n",
    "    )\n",
    "    expected_outputs = model(**inputs).logits\n",
    "    ort_session = ort.InferenceSession(onnx_file_path)\n",
    "    inputs_onnx = {name: inputs[name].numpy() for name in input_names}\n",
    "    outputs = ort_session.run(None, inputs_onnx)\n",
    "    np.testing.assert_allclose(\n",
    "        np.array(outputs[0]), expected_outputs.detach().numpy(), rtol=1e-03, atol=1e-05\n",
    "    )\n",
    "    return outputs\n",
    "classify = lambda sentence: classify_sample(\n",
    "    sentence, model, onnx_file_path, input_names\n",
    ")\n",
    "classify(\"My dog is so ugly\")\n",
    "classify(\"My food is bitter\")\n",
    "classify(\"The film I watched was very entertaining\")\n",
    "classify(\"\")\n",
    "classify(\"I am happy today\")\n",
    "classify(\"Get the latest news from your phone and earn now\")\n",
    "classify(\"Get the inheritance for the prince in Nigeria\")\n",
    "classify(\"You have won a lottery\")\n",
    "classify(\"You have won a lottery and need to pay a fee to claim it\")\n",
    "classify(\"You have won a lottery and need to pay a fee to claim it. Please provide your bank details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the PyTorch Model\n",
    "This section tries running a PyTorch model for text classification using Hugging Face's `pipeline` for comparison with ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_TEXT = \"this is a positive sentence about the movie\"\n",
    "# model.eval()\n",
    "# torch_classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "# torch_results = torch_classifier(INPUT_TEXT)\n",
    "# print(\"TORCH:\", torch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the ONNX Model\n",
    "We attempt to classify the same inputs using the ONNX model that was converted earlier from Hugging Face."
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "#### Trying PyTorch model from Huggingface\n",
      "In this section, we try a text classification task using a pre-trained PyTorch model from Huggingface. We prepare the input text, pass it through the model, and retrieve the classification result."
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 8,
    "metadata": {},
    "outputs": [],
    "source": [
      "# INPUT_TEXT = \"this is a positive sentence about the movie\"\n",
      "# model.eval()\n",
      "# torch_classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
      "# torch_results = torch_classifier(INPUT_TEXT)\n",
      "# print(\"TORCH:\", torch_results)\n",
      "\n",
      "# save_directory = \".\"\n",
      "# onnx_model = ORTModelForSequenceClassification.from_pretrained(\".\", file_name=\"bert_tiny.onnx\")\n",
      "# onnx_classifier = pipeline('text-classification', model=onnx.load(onnx_file_path), tokenizer=tokenizer)\n",
      "# onnx_results = onnx_classifier(INPUT_TEXT)\n",
      "# print(\"ONNX :\", onnx_results)\n",
      "# tokens = tokenizer(\n",
      "#     INPUT_TEXT,\n",
      "#     return_tensors='pt',\n",
      "# )\n",
      "# NUM_INPUT_TOKENS = len(tokens)  # actual number of input tokens\n",
      "\n",
      "# hidden_size = model.config.hidden_size\n",
      "# # To ensure a constant input size, we will always pad or truncate to a fixed size\n",
      "# encoded_input = tokenizer(\n",
      "#     INPUT_TEXT,\n",
      "#     return_tensors='pt',\n",
      "#     padding=\"max_length\",\n",
      "#     truncation=True,\n",
      "#     max_length=hidden_size,\n",
      "# )\n",
      "\n",
      "# # output = model.forward(\n",
      "# #     input_ids=encoded_input[\"input_ids\"],\n",
      "# #     attention_mask=encoded_input[\"attention_mask\"],\n",
      "# # )\n",
      "\n",
      "# encoded_input['input_ids'] = torch.ones_like(encoded_input['input_ids'])\n",
      "\n",
      "# #print(encoded_input)\n",
      "# output = model(**encoded_input)\n",
      "# #print(output)\n",
      "# predicted_classes = torch.argmax(output.logits)\n",
      "# print(\"Predictions are: \", predicted_classes, torch.functional.softmax(output.logits))"
    ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "#### Trying ONNX model converted from Huggingface\n",
      "In this section, we try an ONNX version of the Huggingface model, perform the text classification task, and compare the results with the PyTorch version."
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "import curl\n",
      "\n",
      "curl.init()\n",
      "print(\"ONNX Path: \", onnx_file_path)"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "with open(onnx_file_path, \"rb\") as f:\n",
      "    private_model = curl.nn.from_onnx(f)\n",
      "    print(type(private_model))\n",
      "    private_model.encrypt()\n",
      "    print(type(private_model))\n"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "onnx_model = onnx.load_model(onnx_file_path)\n",
      "for i, node in enumerate(onnx_model.graph.node):\n",
      "    print(f\"[{i}] Operation: {node.op_type}\")\n",
      "    print(f\"{node.name}\")\n",
      "    print(f\"Inputs: {node.input}\")\n",
      "    print(f\"Outputs: {node.output}\")"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "from curl.nn.onnx_executor import ONNXExecutor\n",
      "\n",
      "onnx_executor = ONNXExecutor(onnx_model)\n",
      "result = onnx_executor.forward(\n",
      "    {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
      ")\n",
      "\n",
      "np.testing.assert_allclose(result, expected_outputs.detach().numpy(), rtol=1e-03, atol=1e-05)\n",
      "print(result)"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": 13,
    "metadata": {},
    "outputs": [],
    "source": [
      "node_inputs = onnx_executor.get_node_inputs_by_name(\"/bert/encoder/layer.0/attention/self/Softmax_output_0\")\n"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "\n",
      "opset = set()\n",
      "for i, node in enumerate(onnx_model.graph.node):\n",
      "    # print(f\"[>>>][{node.op_type}]\")\n",
      "    opset.add(node.op_type)\n",
      "    # print(f\"[>>>][{i}]\")\n",
      "    # print(f\"Operation: {node.op_type} (node {i})\")\n",
      "    # print(f\"Inputs: {node.input}\")\n",
      "    # print(f\"Outputs: {node.output}\")\n",
      "    # print(f\"[<<<][{i}]\")\n",
      "print(opset)"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "private_model.eval()\n",
      "private_input_ids = curl.cryptensor(inputs[\"input_ids\"], precision=0)\n",
      "private_attention_mask = curl.cryptensor(inputs[\"attention_mask\"], precision=0)\n",
      "\n",
      "private_input_ids.encoder._precision_bits"
    ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": []
  },
  {
    "cell_type": "code",
    "execution_count": 16,
    "metadata": {},
    "outputs": [],
    "source": [
      "private_model.eval()\n",
      "private_input_ids = curl.cryptensor(inputs[\"input_ids\"], precision=0)\n",
      "private_attention_mask = curl.cryptensor(inputs[\"attention_mask\"], precision=0)\n",
      "private_output = private_model(private_input_ids, private_attention_mask)"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "expected_outputs = model(**inputs).logits\n",
      "diff = expected_outputs - private_output.get_plain_text()\n",
      "print(\"Diff: \", diff)\n",
      "expected_outputs, private_output.get_plain_text()"
    ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": []
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "%%time\n",
      "import onnxruntime as ort\n",
      "\n",
      "l = []\n",
      "\n",
      "def classify_sample(sentence, model, private_model):\n",
      "    inputs = tokenizer(\n",
      "        sentence,\n",
      "        return_tensors=\"pt\",\n",
      "        padding=\"max_length\",\n",
      "        truncation=True,\n",
      "        max_length=hidden_size,\n",
      "    )\n",
      "    expected_outputs = model(**inputs).logits\n",
      "\n",
      "    private_model.eval()\n",
      "    private_input_ids = curl.cryptensor(inputs[\"input_ids\"], precision=0)\n",
      "    private_attention_mask = curl.cryptensor(inputs[\"attention_mask\"], precision=0)\n",
      "    private_output = private_model(private_input_ids, private_attention_mask)\n",
      "    private_output = private_output.get_plain_text().numpy()\n",
      "    print(f\"Expected outputs: {expected_outputs}\")\n",
      "    print(f\"CRYPTEN model outputs: {private_output}\")\n",
      "    np.testing.assert_allclose(expected_outputs.detach().numpy(), private_output, rtol=1e-03, atol=1e-05)\n",
      "    l.append(sentence)\n",
      "    return expected_outputs, private_output\n",
      "\n",
      "classify_sample(\"Sample sentence to classify\", model, private_model)\n"
    ]
  }
    ]
}