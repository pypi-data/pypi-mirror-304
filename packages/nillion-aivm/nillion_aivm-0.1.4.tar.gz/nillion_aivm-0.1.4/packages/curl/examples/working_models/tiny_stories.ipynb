{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiasleys/projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Communicator type:  <class 'curl.communicator.distributed_communicator.DistributedCommunicator'>\n",
      "[<>] Waiting for connections...\n",
      "[<>] DEFAULT ARGS: {'DISTRIBUTED_BACKEND': 'gloo', 'RENDEZVOUS': 'file:///tmp/xcrypten-Vcrypten-rcrypten-wcrypten-Jcrypten-Jcrypten-Ocrypten-Lcrypten-ncrypten-v', 'WORLD_SIZE': 1, 'RANK': 0, 'TTP': False}\n",
      "[Device] LUTs initialized for cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import curl\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "curl.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"roneneldan/TinyStories-1M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a trick to enable tokenizer padding if not yet possible before\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: think could also have a different name like `n_embd` in the config, depending on the model\n",
    "hidden_size = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT = \"the little girl\"\n",
    "\n",
    "tokens = tokenizer(\n",
    "    INPUT_TEXT,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "NUM_INPUT_TOKENS = len(tokens)  # actual number of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure a constant input size, we will always pad or truncate to a fixed size\n",
    "encoded_input = tokenizer(\n",
    "    INPUT_TEXT,\n",
    "    return_tensors='pt',\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=hidden_size,\n",
    ")\n",
    "\n",
    "output = model.forward(\n",
    "    input_ids=encoded_input[\"input_ids\"],\n",
    "    attention_mask=encoded_input[\"attention_mask\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 50257])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape: batch_size x sequence length x vocab size\n",
    "# interpretation: for every token in the sequence, what is the probability distribution over all tokens in the vocab for what the following token is?\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next predicted token is:   was\n",
      "Text completion is:  the little girl was\n"
     ]
    }
   ],
   "source": [
    "# The prediction is the logit argmax of the next token prediction for the last position of the actual input sequence\n",
    "\n",
    "predicted_token_id = torch.argmax(output.logits[:, NUM_INPUT_TOKENS, :], dim=-1)\n",
    "print(\"Next predicted token is: \", tokenizer.decode(predicted_token_id))\n",
    "print(\"Text completion is: \", INPUT_TEXT + tokenizer.decode(predicted_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiasleys/projects/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "/Users/mathiasleys/projects/.venv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/Users/mathiasleys/projects/.venv/lib/python3.12/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:216: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
      "/Users/mathiasleys/projects/aivm-curl/curl/nn/onnx_converter.py:176: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n"
     ]
    }
   ],
   "source": [
    "private_model = curl.nn.from_pytorch(model, encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph encrypted module"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_model.encrypt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_input = curl.cryptensor(encoded_input[\"input_ids\"], precision=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_output = private_model.forward(private_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next predicted token is:  urious\n",
      "Text completion is:  the little girlurious\n"
     ]
    }
   ],
   "source": [
    "next_token_id = torch.argmax(private_output.get_plain_text()[:, NUM_INPUT_TOKENS, :], dim=-1)\n",
    "print(\"Next predicted token is: \", tokenizer.decode(next_token_id))\n",
    "print(\"Text completion is: \", INPUT_TEXT + tokenizer.decode(next_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: curl.nn.module.Graph,\n",
    "    input_tokens: curl.mpc.mpc.MPCTensor,\n",
    "    sequence_length: int,\n",
    "    max_new_tokens: int,\n",
    ") -> list[curl.mpc.mpc.MPCTensor]:\n",
    "\n",
    "    sequence = input_tokens.clone()\n",
    "\n",
    "    generated_tokens = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Perform forward pass & compute logits\n",
    "        [logits] = model.forward(sequence)  # NOTE: assumption = batch size of 1\n",
    "        # Select logit of last token in input sequence (which hold logits over the vocab for next token)\n",
    "        next_logit = logits[sequence_length - 1]\n",
    "        # Compute argmax to determine next token\n",
    "        next_token_id = next_logit.argmax(one_hot=False)\n",
    "        generated_tokens.append(next_token_id)\n",
    "        next_token_id = next_token_id._tensor.data.item()  # NOTE: if we encoded the IDs as rationals, this would need to be upscaled\n",
    "        # Check if we reached the maximum sequence length\n",
    "        # Here this is equal to the logits dim because we padded / truncated the sequence to this maximum length\n",
    "        if sequence_length >= len(logits):\n",
    "            # We make room for the next token by discarding the first token in the sequence\n",
    "            raw_truncated_input_tokens = raw_truncated_input_tokens._tensor.data[1:]\n",
    "            sequence._tensor.data = torch.cat(\n",
    "                (\n",
    "                    raw_truncated_input_tokens,\n",
    "                    torch.tensor([[next_token_id]]),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )  \n",
    "        else:\n",
    "            # Replace padding token by new token & update sequence length bc input sequence is now 1 token longer\n",
    "            sequence._tensor.data[0][sequence_length] = next_token_id\n",
    "            sequence_length += 1\n",
    "\n",
    "    return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: the little girl\n",
      "Output sequence: the little girlokes again emergencies\n"
     ]
    }
   ],
   "source": [
    "[input_token_ids] = private_input.reveal().int()\n",
    "input_seq = tokenizer.decode(\n",
    "    input_token_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "print(\"Input sequence:\", input_seq)\n",
    "\n",
    "private_generated_token_ids = generate(\n",
    "    model=private_model,\n",
    "    input_tokens=private_input,\n",
    "    sequence_length=NUM_INPUT_TOKENS,\n",
    "    max_new_tokens=3,\n",
    ")\n",
    "\n",
    "# TODO: There has to be a cleaner way to structure this\n",
    "generated_token_ids = [\n",
    "    x.reveal().int().item()\n",
    "    for x in private_generated_token_ids\n",
    "]\n",
    "generated_seq = tokenizer.decode(\n",
    "    generated_token_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "output_seq = input_seq + generated_seq\n",
    "print(\"Output sequence:\", output_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
