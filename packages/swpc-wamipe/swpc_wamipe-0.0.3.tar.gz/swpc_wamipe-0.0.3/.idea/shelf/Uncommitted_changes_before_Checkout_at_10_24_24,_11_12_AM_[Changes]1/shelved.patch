Index: src/wam_api/lru_cache.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from collections import OrderedDict\r\nimport os\r\nimport json\r\nimport asyncio\r\nimport aiohttp\r\nimport requests\r\n\r\nclass LRU_Cache():\r\n    # Define Required Constants\r\n    FILE_SIZE = 3  # size of each file in MB\r\n    CACHE_PATH = 'wamapi_filecache'  # You can adjust the path as needed\r\n    CACHE_METADATA = f'{CACHE_PATH}/metadata.json'\r\n\r\n    def __init__(self, max_size=100, s3bucket='noaa-nws-wam-ipe-pds'):\r\n        self.max_size = max_size  # Maximum cache size in MB\r\n        self.s3bucket = s3bucket\r\n\r\n        # Initialize cache directory and metadata\r\n        if os.path.exists(self.CACHE_METADATA):\r\n            with open(self.CACHE_METADATA, 'r') as metadata_file:\r\n                self.file_map = json.load(metadata_file, object_pairs_hook=OrderedDict)\r\n                self.current_size = len(self.file_map.keys()) * self.FILE_SIZE\r\n        else:\r\n            os.makedirs(self.CACHE_PATH, exist_ok=True)  # Create directory if it doesn't exist\r\n            self.file_map = OrderedDict()   # Maps file paths to their local file paths\r\n            self.current_size = 0\r\n\r\n        self.lock = asyncio.Lock()  # Async lock for thread safety\r\n\r\n    def get_file(self, file_path):\r\n        if file_path in self.file_map:\r\n            print(f'File IN cache: {file_path}')\r\n            self.file_map.move_to_end(file_path)    # Mark file as recently used\r\n            self.update_metadata()\r\n            return self.file_map[file_path]\r\n        else:\r\n            print(f'File NOT in cache: {file_path}... retrieving from S3')\r\n            if self.put_file(file_path):\r\n                self.file_map.move_to_end(file_path)    # Mark file as recently used\r\n                self.update_metadata()\r\n                return self.file_map[file_path]\r\n            else:\r\n                self.update_metadata()\r\n                return None\r\n\r\n    def put_file(self, file_path):\r\n        # Remove LRU files if necessary\r\n        while self.current_size + self.FILE_SIZE > self.max_size:\r\n            self.remove_lru_file()\r\n\r\n        url = f\"https://{self.s3bucket}.s3.amazonaws.com/{file_path}\"\r\n        response = requests.get(url)\r\n\r\n        if response.status_code == 200:\r\n            # Create a valid local file path\r\n            output_path = os.path.join(\r\n                self.CACHE_PATH, f'{file_path.replace(\".\", \"_\").replace(\"/\", \"_\")}.nc')\r\n\r\n            with open(output_path, 'wb') as file:\r\n                file.write(response.content)\r\n\r\n            self.file_map[file_path] = output_path\r\n            self.current_size += self.FILE_SIZE  # Update cache size\r\n            print(f'File added to cache: {file_path}')\r\n            return True\r\n        else:\r\n            print(f\"Failed to retrieve file from S3: {file_path}\")\r\n            return False\r\n\r\n    async def get_file_async(self, file_path):\r\n        async with self.lock:\r\n            if file_path in self.file_map:\r\n                print(f'File IN cache: {file_path}')\r\n                self.file_map.move_to_end(file_path)    # Mark file as recently used\r\n                self.update_metadata()\r\n                return self.file_map[file_path]\r\n            \r\n        print(f'File NOT in cache: {file_path}... retrieving from S3')\r\n        \r\n        # Remove LRU files if necessary (outside the lock)\r\n        while self.current_size + self.FILE_SIZE > self.max_size:\r\n            await self.remove_lru_file_async()\r\n\r\n        success = await self.put_file_async(file_path)\r\n        \r\n        async with self.lock:\r\n            if success:\r\n                self.file_map.move_to_end(file_path)\r\n                self.update_metadata()\r\n                return self.file_map[file_path]\r\n            else:\r\n                self.update_metadata()\r\n                return None\r\n\r\n    async def put_file_async(self, file_path):\r\n        url = f\"https://{self.s3bucket}.s3.amazonaws.com/{file_path}\"\r\n\r\n        async with aiohttp.ClientSession() as session:\r\n            async with session.get(url) as response:\r\n                if response.status == 200:\r\n                    content = await response.read()\r\n                    # Create a valid local file path\r\n                    output_path = os.path.join(\r\n                        self.CACHE_PATH, f'{file_path.replace(\".\", \"_\").replace(\"/\", \"_\")}.nc')\r\n\r\n                    with open(output_path, 'wb') as file:\r\n                        file.write(content)\r\n\r\n                    self.file_map[file_path] = output_path\r\n                    self.current_size += self.FILE_SIZE\r\n                    print(f'File added to cache: {file_path}')\r\n                    return True\r\n                else:\r\n                    print(f\"Failed to retrieve file from S3: {file_path}\")\r\n                    return False\r\n\r\n    async def remove_lru_file_async(self):\r\n        async with self.lock:\r\n            # Remove the least recently used file from the cache\r\n            lru_file_path, local_path = self.file_map.popitem(last=False)\r\n        \r\n        if os.path.exists(local_path):\r\n            await asyncio.to_thread(os.remove, local_path)\r\n            print(f'Removed LRU file: {lru_file_path}')\r\n        else:\r\n            print('ERROR - Attempted to remove non-existent file')\r\n\r\n        async with self.lock:\r\n            self.current_size -= self.FILE_SIZE\r\n\r\n    def remove_lru_file(self):\r\n        # Remove the least recently used file from the cache\r\n        lru_file_path, local_path = self.file_map.popitem(last=False)\r\n        if os.path.exists(local_path):\r\n            os.remove(local_path)\r\n            print(f'Removed LRU file: {lru_file_path}')\r\n        else:\r\n            print('ERROR - Attempted to remove non-existent file')\r\n\r\n        self.current_size -= self.FILE_SIZE\r\n\r\n    def update_metadata(self):\r\n        # Update cache metadata\r\n        with open(self.CACHE_METADATA, 'w') as metadata_file:\r\n            json.dump(self.file_map, metadata_file, indent=4)\r\n\r\n    def print_cache(self):\r\n        print(f'Max size: {self.max_size} MB, Current size: {self.current_size} MB')\r\n        print(f'Cache contains {len(self.file_map.keys())} entries:')\r\n        for key in self.file_map.keys():\r\n            print(f'\\t- {key}')\r\n\r\n    def clear_cache(self):\r\n        # Remove all files from the cache directory\r\n        for file_path in self.file_map.values():\r\n            if os.path.exists(file_path):\r\n                os.remove(file_path)\r\n                print(f'Removed file: {file_path}')\r\n            else:\r\n                print(f'WARNING - File not found: {file_path}')\r\n\r\n        # Clear the file map and reset current size\r\n        self.file_map.clear()\r\n        self.current_size = 0\r\n\r\n        # Update the metadata file\r\n        self.update_metadata()\r\n\r\n        print('Cache cleared successfully')\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/wam_api/lru_cache.py b/src/wam_api/lru_cache.py
--- a/src/wam_api/lru_cache.py	
+++ b/src/wam_api/lru_cache.py	
@@ -29,12 +29,12 @@
 
     def get_file(self, file_path):
         if file_path in self.file_map:
-            print(f'File IN cache: {file_path}')
+            # print(f'File IN cache: {file_path}')
             self.file_map.move_to_end(file_path)    # Mark file as recently used
             self.update_metadata()
             return self.file_map[file_path]
         else:
-            print(f'File NOT in cache: {file_path}... retrieving from S3')
+            # print(f'File NOT in cache: {file_path}... retrieving from S3')
             if self.put_file(file_path):
                 self.file_map.move_to_end(file_path)    # Mark file as recently used
                 self.update_metadata()
@@ -70,12 +70,12 @@
     async def get_file_async(self, file_path):
         async with self.lock:
             if file_path in self.file_map:
-                print(f'File IN cache: {file_path}')
+                # print(f'File IN cache: {file_path}')
                 self.file_map.move_to_end(file_path)    # Mark file as recently used
                 self.update_metadata()
                 return self.file_map[file_path]
             
-        print(f'File NOT in cache: {file_path}... retrieving from S3')
+        # print(f'File NOT in cache: {file_path}... retrieving from S3')
         
         # Remove LRU files if necessary (outside the lock)
         while self.current_size + self.FILE_SIZE > self.max_size:
